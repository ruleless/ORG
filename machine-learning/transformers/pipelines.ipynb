{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipelines\n",
    "\n",
    "### text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipelines as pl\n",
    "\n",
    "analyzer = pl.pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "inputs = [\"Hello world\", \"I hate you\", \"It's a beautiful day\"]\n",
    "preds = analyzer(inputs)\n",
    "for i, pred in enumerate(preds):\n",
    "    print(\"sentence: \", inputs[i])\n",
    "    print(\"inference: \", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", \"dslim/bert-base-NER\")\n",
    "\n",
    "inputs = [\n",
    "    \"My name is Wolfgang and I live in Berlin\"\n",
    "]\n",
    "outputs = ner(inputs)\n",
    "for i, res in enumerate(outputs):\n",
    "    print(\"%s\\n%s\" % (inputs[i], res[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# extractive QA\n",
    "qa = pipeline(\"question-answering\", model=\"uer/roberta-base-chinese-extractive-qa\")\n",
    "\n",
    "inputs = {\n",
    "    \"question\": \"囚徒的作者是谁\",\n",
    "    \"context\": \"普希金从那里学习人民的语言，吸取了许多有益的养料，这一切对普希金后来的创作产生了很大的影响。这两年里，普希金创作了不少优秀的作品，如《囚徒》、《致大海》、《致凯恩》和《假如生活欺骗了你》等几十首抒情诗，叙事诗《努林伯爵》，历史剧《鲍里斯·戈都诺夫》，以及《叶甫盖尼·奥涅金》前六章。\"\n",
    "}\n",
    "outputs = qa(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sum = pipeline(\"summarization\", model=\"csebuetnlp/mT5_multilingual_XLSum\")\n",
    "input = \"并查集是一种树型的数据结构，用于处理不相交集合的合并及查询问题，在使用中常常以森林来表示。\"\n",
    "print(sum(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipelines as pl\n",
    "\n",
    "inputs = [\n",
    "    \"你好，世界\", \"今天是一个好天气\", \"手写数字识别\",\n",
    "    \"将递归函数转换为非递归函数的一般方法是使用栈来模拟递归调用的过程。具体来说，可以将递归函数中的每个递归调用转换为将参数压入栈中，并在循环中取出栈顶元素进行处理，直到栈为空为止。\",\n",
    "]\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "# model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tr = pl.pipeline(\"translation\", model=model_name)\n",
    "res = tr(inputs, src_lang=\"zh\", tgt_lang=\"eng_Latn\")\n",
    "for i in range(len(inputs)):\n",
    "    print(\"%s\\n%s\\n\"%(inputs[i], res[i][\"translation_text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "inputs = [\n",
    "    \"你好，世界\", \"并查集是一种树型的数据结构，用于处理不相交集合的合并及查询问题，在使用中常常以森林来表示。\",\n",
    "]\n",
    "\n",
    "tf_batch = tokenizer(inputs, padding=True, truncation=True, max_length=512, return_tensors=\"tf\")\n",
    "input_ids = tf_batch.input_ids\n",
    "outputs = model.generate(input_ids, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
    "\n",
    "for i, output_ids in enumerate(outputs):\n",
    "    print(inputs[i])\n",
    "    print(tokenizer.decode(output_ids))\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
