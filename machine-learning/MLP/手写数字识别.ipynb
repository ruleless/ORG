{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备\n",
    "\n",
    "首先，我们需要准备训练数据，我们有以下选择：\n",
    "\n",
    "1. 使用sklearn中的手写数字（8*8）；\n",
    "2. 使用 [tfds](https://github.com/tensorflow/datasets/tree/master) 中的手写数字(28*28) [mnist](https://www.tensorflow.org/datasets/catalog/mnist)。\n",
    "\n",
    "sklean 中的数字的分辨率比较差，人工识别可能都有困难；我们使用 tfds 中的 mnist 数据集。\n",
    "\n",
    "另外，tfds 使用手册可见 [TensorFlow Datasets](https://www.tensorflow.org/datasets/overview?hl=zh-cn)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# sklearn中的手写数据集\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "pyplot.gray()\n",
    "pyplot.matshow(digits.images[8])\n",
    "# pyplot.matshow(digits.images[16])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# tensorflow中的手写数据集\n",
    "mnist, info = tfds.load(\"mnist\", split=\"train\", shuffle_files=True, with_info=True)\n",
    "\n",
    "# 通过pyplot呈现\n",
    "# plt.gray()\n",
    "# for example in tfds.as_numpy(mnist.take(3)):\n",
    "#     print(example.keys())\n",
    "#     plt.matshow(example[\"image\"])\n",
    "\n",
    "# 通过pandas.DataFrame呈现\n",
    "# tfds.as_dataframe(mnist.take(5), info)\n",
    "\n",
    "# 调用tfds.show_examples呈现\n",
    "tfds.show_examples(mnist.take(2), info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过深度学习实现手写数字识别\n",
    "\n",
    "### 通过keras API实现关于手写数字识别的深度学习\n",
    "\n",
    "主要步骤是：\n",
    "\n",
    "1. 通过 `keras.Sequential` 和 `keras.layers.Dense` 创建模型。需要指定模型的层数、每层的节点数以及每层的激活函数。\n",
    "2. 为模型指定损失函数、优化器和指标。损失函数用于衡量模型预测值偏离实际值的程度，在训练过程中我们的目标是将损失最小化；优化器决定如何基于损失函数对神经网络进行更新；指标是衡量成功的标准，在训练和验证过程中需要对其进行监控，与损失不同，训练过程不会直接对指标进行优化。\n",
    "3. 利用训练数据对模型进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 加载训练数据和测试数据\n",
    "(orig_train_images, train_labels), (orig_test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "xx = np.array([])\n",
    "assert(orig_train_images.shape == (60000, 28, 28))\n",
    "assert(train_labels.shape == (60000,))\n",
    "assert(orig_test_images.shape == (10000, 28, 28))\n",
    "assert(test_labels.shape == (10000,))\n",
    "train_images = orig_train_images.reshape(60000, 28*28)\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = orig_test_images.reshape(10000, 28*28)\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "# 构建深度学习模型\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(512, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# 编译，即：为模型指定损失函数和优化器\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# 训练\n",
    "model.fit(train_images[0:10000], train_labels[0:10000], epochs=5, batch_size=128)\n",
    "\n",
    "# 预测\n",
    "start_idx = np.random.randint(0, 10000-10)\n",
    "predictions_as_prop = model.predict(test_images[start_idx:start_idx+10])\n",
    "predictions = []\n",
    "for a in predictions_as_prop:\n",
    "    predictions.append(np.argmax(a))\n",
    "\n",
    "plt.gray()\n",
    "fig, axes = plt.subplots(2, 5)\n",
    "fig.set_size_inches(5*2.5, 2*2.5)\n",
    "for i in range(10):\n",
    "    axes[i//5, i%5].matshow(orig_test_images[start_idx+i])\n",
    "plt.show()\n",
    "\n",
    "print(\"%+12s:\"%\"predictions\", np.array(predictions))\n",
    "print(\"%+12s:\"%\"real\", test_labels[start_idx:start_idx+10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于TensorFlow从头开始实现深度学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 简单的层对象的实现\n",
    "class SimpleDense:\n",
    "    def __init__(self, units, activation) -> None:\n",
    "        self.activation = activation\n",
    "        self.units = units\n",
    "        self.built = False\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"weights:%s\\nbias:%s\" % (self.W.__repr__(), self.b.__repr__())\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        if not self.built:\n",
    "            self.built = True\n",
    "            self.build(inputs.shape[-1])\n",
    "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
    "    \n",
    "    def build(self, input_size):\n",
    "        output_size = self.units\n",
    "\n",
    "        # 通过随机值来初始化权重(weight)\n",
    "        self.W = tf.Variable(tf.random.uniform((input_size, output_size), minval=0., maxval=0.1))\n",
    "\n",
    "        # 初始化偏置(bias)\n",
    "        self.b = tf.Variable(tf.zeros((output_size,)))\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "# 模型对象\n",
    "class SimpleSequential:\n",
    "    def __init__(self, layers) -> None:\n",
    "        assert(len(layers) > 0)\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        w = []\n",
    "        for layer in self.layers:\n",
    "            w += layer.weights\n",
    "        return w\n",
    "\n",
    "# 权重更新\n",
    "# learning_rate = 0.01\n",
    "# def update_weights(gradients, weights):\n",
    "#     for g, w in zip(gradients, weights):\n",
    "#         w.assign_sub(g*learning_rate)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01)\n",
    "def update_weights(gradients, weights):\n",
    "    optimizer.apply_gradients(zip(gradients, weights))\n",
    "\n",
    "# 一次训练过程\n",
    "def one_traning_step(model: SimpleSequential, images_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images_batch)\n",
    "        per_sample_losses = keras.losses.sparse_categorical_crossentropy(labels_batch, predictions)\n",
    "        average_loss = tf.reduce_mean(per_sample_losses)\n",
    "    gradients = tape.gradient(average_loss, model.weights)\n",
    "    update_weights(gradients, model.weights)\n",
    "    return average_loss\n",
    "\n",
    "# 批量数据生成器\n",
    "class BatchGenerator:\n",
    "    def __init__(self, images, labels, batch_size) -> None:\n",
    "        assert(len(images) == len(labels))\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(images)/batch_size)\n",
    "        self.cur_index = 0\n",
    "\n",
    "    def next(self):\n",
    "        start_idx, end_idx = self.cur_index, self.cur_index+self.batch_size\n",
    "        self.cur_index += self.batch_size\n",
    "        return self.images[start_idx:end_idx], self.labels[start_idx:end_idx]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.cur_index = 0\n",
    "\n",
    "# 完整的训练过程\n",
    "def fit(model: SimpleSequential, images, labels, epoches, batch_size=128):\n",
    "    for epoch_counter in range(epoches):\n",
    "        print(f\"Epoch {epoch_counter+1}\")\n",
    "        batch_generotor = BatchGenerator(images, labels, batch_size)\n",
    "        for batch_counter in range(batch_generotor.num_batches):\n",
    "            images_batch, labels_batch = batch_generotor.next()\n",
    "            loss = one_traning_step(model, images_batch, labels_batch)\n",
    "            if batch_counter%100 == 0:\n",
    "                print(f\"loss at batch {batch_counter}: {loss}\")\n",
    "\n",
    "# 计算模型准确率\n",
    "def accuracy(model:SimpleSequential, images, labels, batch_size=128) -> float:\n",
    "    batch_generotor = BatchGenerator(images, labels, batch_size)\n",
    "    for batch_counter in range(batch_generotor.num_batches):\n",
    "        images_batch, labels_batch = batch_generotor.next()\n",
    "        predictions = np.argmax(model(images_batch), axis=1)\n",
    "        y = (predictions == labels_batch)\n",
    "        y = y.astype(\"float32\")\n",
    "        return tf.reduce_mean(y)\n",
    "\n",
    "# 加载训练数据和测试数据\n",
    "(orig_train_images, train_labels), (orig_test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "xx = np.array([])\n",
    "assert(orig_train_images.shape == (60000, 28, 28))\n",
    "assert(train_labels.shape == (60000,))\n",
    "assert(orig_test_images.shape == (10000, 28, 28))\n",
    "assert(test_labels.shape == (10000,))\n",
    "train_images = orig_train_images.reshape(60000, 28*28)\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = orig_test_images.reshape(10000, 28*28)\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "# 构建模型\n",
    "model = SimpleSequential([\n",
    "    SimpleDense(512, keras.activations.relu),\n",
    "    SimpleDense(10, keras.activations.softmax)\n",
    "])\n",
    "\n",
    "# 训练模型\n",
    "fit(model, train_images, train_labels, 5, 128)\n",
    "\n",
    "# 模型准确率\n",
    "print(\"accuracy on test images: %.2f%%100\" % (accuracy(model, test_images, test_labels)*100))\n",
    "\n",
    "# 预测\n",
    "start_idx = np.random.randint(0, 10000-10)\n",
    "predictions_as_arr = model(test_images[start_idx:start_idx+10])\n",
    "predictions = np.argmax(predictions_as_arr, axis=1)\n",
    "\n",
    "plt.gray()\n",
    "fig, axes = plt.subplots(2, 5)\n",
    "fig.set_size_inches(5*2.5, 2*2.5)\n",
    "for i in range(10):\n",
    "    axes[i//5, i%5].matshow(orig_test_images[start_idx+i])\n",
    "plt.show()\n",
    "\n",
    "print(\"%+12s:\"%\"predictions\", np.array(predictions))\n",
    "print(\"%+12s:\"%\"real\", test_labels[start_idx:start_idx+10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
