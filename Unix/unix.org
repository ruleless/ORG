* Unix 环境编程技术概览
  基于 Unix 环境的应用程序开发主要涉及的技术或知识点包括：Unix 文件系统、进程、进程 IPC、线程、socket 编程，另外，对于任何平台， I/O 操作总是不可或缺的，我们也可以将 I/O 独立出来列为一个技术点。
  |---------------+----------------------------------------------------------------------------------------|
  | Unix 文件系统 | 1. 文件 I/O                                                                            |
  |               | 2. 文件系统                                                                            |
  |               | 3. 文件属性管理                                                                        |
  |               | 4. 文件记录锁                                                                          |
  |---------------+----------------------------------------------------------------------------------------|
  | 进程          | 1. 进程概要                                                                            |
  |               | 2. 进程控制                                                                            |
  |               | 3. 进程会话                                                                            |
  |               | 4. 信号                                                                                |
  |               | 5. 守护进程                                                                            |
  |---------------+----------------------------------------------------------------------------------------|
  | 进程 IPC      | 1. 管道和FIFO(也称有名管道)                                                            |
  |               | 2. 消息队列：Posix 消息队列、System V消息队列                                          |
  |               | 3. 共享内存：内存映射 I/O、Posix 共享内存区、System V 共享内存                         |
  |---------------+----------------------------------------------------------------------------------------|
  | 线程          | 1. 线程控制原语                                                                        |
  |               | 2. 同步机制(也可用于进程同步)：互斥锁、条件变量、读写锁、信号量                        |
  |---------------+----------------------------------------------------------------------------------------|
  | socket 编程   | 1. 网络地址结构                                                                        |
  |               | 2. 基于 TCP 协议的 socket 编程接口                                                     |
  |               | 3. 基于 UDP 协议的 socket 编程接口                                                     |
  |---------------+----------------------------------------------------------------------------------------|
  | I/O           | 1. 五种 I/O 模型：阻塞式 I/O、非阻塞式 I/O、I/O 复用、信号驱动式 I/O、异步 I/O         |
  |               | 2. I/O 函数汇总：read/write、readv/writev、recvfrom/sendto、recv/send、recvmsg/sendmsg |
  |---------------+----------------------------------------------------------------------------------------|
* I/O
** epoll
*** 概要
	Linux I/O 多路复用技术在比较多的 TCP 网络服务器中有使用，即比较多的用到 select 函数。Linux 2.6 内核中有提高网络 I/O 性能的新方法，即 epoll 。
*** 为什么select落后？
	首先，在Linux内核中，select 所用到的 FD_SET 是有限的，即内核中有个参数 __FD_SETSIZE 定义了每个 FD_SET 的句柄个数，在我用的 2.6.15-25-386 内核中，该值是 1024，搜索内核源代码得到：
	#+begin_src c
      // include/linux/posix_types.h:
      #define __FD_SETSIZE         1024
	#+end_src

	也就是说，如果想要同时检测 1025 个句柄的可读状态是不可能用 select 实现的。或者同时检测 1025 个句柄的可写状态也是不可能的。其次，内核中实现 select 是使用轮询方法，即每次检测都会遍历所有 FD_SET 中的句柄，显然，select 函数的执行时间与 FD_SET 中句柄的个数有一个比例关系，即 select 要检测的句柄数越多就会越费时。当然，在前文中我并没有提及 poll 方法，事实上用 select 的朋友一定也试过 poll，我个人觉得 select 和 poll 大同小异，个人偏好于用 select 而已。
*** 内核中提高 I/O 性能的新方法 epoll
	epoll 是什么？按照 man 手册的说法：是为处理大批量句柄而作了改进的 poll。要使用 epoll 只需要以下的三个系统函数调用：epoll_create， epoll_ctl， epoll_wait。

	先介绍 2 本书。《The Linux Networking Architecture--Design and Implementation of Network Protocols in the Linux Kernel》，以 2.4 内核讲解 Linux TCP/IP 实现，相当不错。作为一个现实世界中的实现，很多时候你必须作很多权衡，这时候参考一个久经考验的系统更有实际意义。举个例子，linux内核中 sk_buff 结构为了追求速度和安全，牺牲了部分内存，所以在发送 TCP 包的时候，无论应用层数据多大，sk_buff 最小也有 272 的字节。其实对于 socket 应用层程序来说，另外一本书《UNIX Network Programming Volume 1》意义更大一点。2003年的时候，这本书出了最新的第3版本，不过主要还是修订第2版本。其中第6章《I/O Multiplexing》是最重要的，Stevens给出了网络 I/O 的基本模型。在这里最重要的莫过于 select 模型和 Asynchronous I/O模型。从理论上说，AIO 似乎是最高效的，你的IO操作可以立即返回，然后等待 os 告诉你 I/O 操作完成。但是一直以来，如何实现就没有一个完美的方案。最著名的 windows 完成端口实现的 AIO，实际上也只是内部用线程池实现的罢了，最后的结果是 I/O有个线程池，你的应用程序也需要一个线程池。很多文档其实已经指出了这引发的线程 context-switch 所带来的代价。在 linux 平台上，关于网络 AIO 一直是改动最多的地方，2.4 的年代就有很多 AIO 内核 patch，最著名的应该算是 SGI。但是一直到2.6内核发布，网络模块的 AIO 一直没有进入稳定内核版本(大部分都是使用用户线程模拟方法，在使用了 NPTL 的 linux 上面其实和 windows 的完成端口基本上差不多了)。2.6 内核所支持的 AIO 特指磁盘的AIO，支持 io_submit(), io_getevents() 以及对 Direct I/O 的支持(即：就是绕过 VFS 系统 buffer 直接写硬盘，对于流服务器在内存平稳性上有相当的帮助)。

	所以，剩下的 select 模型基本上就成为我们在 linux 上面的唯一选择，其实，如果加上 no-block socket 的配置，可以完成一个"伪" AIO 的实现，只不过推动力在于你而不是 os 而已。不过传统的 select/poll 函数有着一些无法忍受的缺点，所以改进一直是 2.4-2.5 开发版本内核的任务，包括 /dev/poll，realtime signal 等等。最终，Davide Libenzi 开发的 epoll 进入2.6内核成为正式的解决方案。
*** epoll 的优点
	* *支持一个进程打开大数目的 socket 描述符(fd)* select 最不能忍受的是一个进程所打开的 fd 是有一定限制的，由 FD_SETSIZE 设置， 默认值是 2048。对于那些需要支持上万连接数目的 IM 服务器来说显然太少了。这时候你一是可以选择修改这个宏然后重新编译内核，不过资料也同时指出这样会带来网络效率的下降；二是可以选择多进程的解决方案(传统的 Apache 方案)，不过虽然 linux 上面创建进程的代价比较小，但仍旧是不可忽视的，加上进程间数据同步远比不上线程间同步高效，所以这也不是一种完美的方案。不过 epoll 没有这个限制，它所支持的 fd 上限是最大可以打开文件的数目，这个数字一般远大于 select 所支持的2048。举个例子，在 1GB 内存的机器上大约是 10 万左右，具体数目可以 cat /proc/sys/fs/file-max 察看，一般来说这个数目和系统内存关系很大。
	* *IO效率不随 fd 数目增加而线性下降* 传统 select/poll 的另一个致命弱点就是当你拥有一个很大的 socket 集合，由于网络得延时，使得任一时间只有部分的 socket 是"活跃" 的，而 select/poll 每次调用都会线性扫描全部的集合，导致效率呈现线性下降。但是 epoll 不存在这个问题，它只会对"活跃"的 socket 进行操作，这是因为在内核实现中 epoll 是根据每个 fd 上面的 callback 函数实现的。于是，只有"活跃"的 socket 才会主动去调用 callback 函数，其他 idle 状态的 socket 则不会，在这点上，epoll实现了一个"伪" AIO，因为这时候推动力在 os 内核。在一些 benchmark 中，如果所有的 socket 基本上都是活跃的，比如一个高速LAN环境，epoll 也不比 select/poll 低多少效率，但若过多地调用 epoll_ctl，效率稍微有些下降。然而一旦使用 idle connections 模拟 WAN 环境，那么 epoll 的效率就远在 select/poll 之上了。
	* *使用 mmap 加速内核与用户空间的消息传递* 这点实际上涉及到 epoll 的具体实现。无论是 select、poll 还是 epoll 都需要内核把 fd 消息通知给用户空间，如何避免不必要的内存拷贝就显得很重要，在这点上，epoll 是通过内核于用户空间 mmap 同一块内存实现的。而如果你像我一样从2.5内核就开始关注 epoll 的话，一定不会忘记手 工 mmap 这一步的。
	* *内核微调* 这一点其实不算 epoll 的优点，而是整个 linux 平台的优点。也许你可以怀疑 linux 平台，但是你无法回避 linux 平台赋予你微调内核的能力。比如，内核 TCP/IP 协议栈使用内存池管理 sk_buff 结构，可以在运行期间动态地调整这个内存 pool(skb_head_pool) 的大小（通过 echo XXXX>/proc/sys/net/core/hot_list_length 来完成）。再比如 listen 函数的第2个参数(TCP完成3次握手的数据包队列长度)，也可以根据你平台内存大小来动态调整。甚至可以在一个数据包面数目巨大但同时每个数据包本身大小却很小的特殊系统上尝试最新的 NAPI 网卡驱动架构。
*** epoll 的工作模式
    令人高兴的是，linux2.6 内核的 epoll 比其 2.5 开发版本的 /dev/epoll 简洁了许多，所以，大部分情况下，强大的东西往往是简单的。唯一有点麻烦的是 epoll 有2种工作方式：LT 和 ET。

    LT(level triggered) 是缺省的工作方式，并且同时支持 block 和 no-block socket。在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的 fd 进行 I/O 操作。如果你不作任何操作，内核还是会继续通知你的，所以，这种模式编程出错误可能性要小一点。传统的 select/poll 都是这种模型的代表。

	ET(edge-triggered) 是高速工作方式，只支持 no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核就通过 epoll 告诉你，然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作而导致那个文件描述符不再是就绪状态(比如你在发送，接收或是接受请求，或者发送接收的数据少于一定量时导致了一个 EWOULDBLOCK 错误)。但是请注意，如果一直不对这个 fd 作 I/O 操作(从而导致它再次变成未就绪)，内核就不会发送更多的通知。不过在TCP协议中，ET 模式的加速效用仍需要更多的 benchmark 确认。

	epoll 只有 epoll_create、epoll_ctl、epoll_wait 3个系统调用。
*** epoll 的使用方法
	epoll用到的所有函数都是在头文件 sys/epoll.h 中声明的，下面简要说明所用到的数据结构和函数。

	所用到的数据结构：
	#+begin_src c
      typedef union epoll_data
      {
          void *ptr;
          int fd;
          __uint32_t u32;
          __uint64_t u64;
      } epoll_data_t;

      struct epoll_event
      {
          __uint32_t events; /* Epoll events */
          epoll_data_t data; /* User data variable */
      };
	#+end_src
	结构体 epoll_event 被用于注册所感兴趣的事件和回传所发生待处理的事件，而 epoll_data 联合体用来保存触发事件的某个文件描述符相关的数据。例如一个 client 连接到服务器，服务器通过调用 accept 函数可以得到于这个 client 对应的 socket 文件描述符，可以把这文件描述符赋给 epoll _data 的 fd 字段，以便后面的读写操作在这个文件描述符上进行。epoll_event 结构体的 events 字段是表示感兴趣的事件和被触发的事件，可能的取值为：
	#+begin_src c
      enum EPOLL_EVENTS
      {
          EPOLLIN = 0x001,
      #define EPOLLIN EPOLLIN // 表示对应的文件描述符可以读；
          EPOLLPRI = 0x002,
      #define EPOLLPRI EPOLLPRI // 表示对应的文件描述符有紧急的数据可读；
          EPOLLOUT = 0x004,
      #define EPOLLOUT EPOLLOUT // 表示对应的文件描述符可以写；
          EPOLLRDNORM = 0x040,
      #define EPOLLRDNORM EPOLLRDNORM
          EPOLLRDBAND = 0x080,
      #define EPOLLRDBAND EPOLLRDBAND
          EPOLLWRNORM = 0x100,
      #define EPOLLWRNORM EPOLLWRNORM
          EPOLLWRBAND = 0x200,
      #define EPOLLWRBAND EPOLLWRBAND
          EPOLLMSG = 0x400,
      #define EPOLLMSG EPOLLMSG
          EPOLLERR = 0x008,
      #define EPOLLERR EPOLLERR // 表示对应的文件描述符发生错误；
          EPOLLHUP = 0x010,
      #define EPOLLHUP EPOLLHUP // 表示对应的文件描述符被挂断；
          EPOLLRDHUP = 0x2000,
      #define EPOLLRDHUP EPOLLRDHUP
          EPOLLWAKEUP = 1u << 29,
      #define EPOLLWAKEUP EPOLLWAKEUP
          EPOLLONESHOT = 1u << 30,
      #define EPOLLONESHOT EPOLLONESHOT
          EPOLLET = 1u << 31
      #define EPOLLET EPOLLET // 表示对应的文件描述符有事件发生；
      };
	#+end_src

	所用到的函数：
	* epoll_create
	  #+begin_src c
        int epoll_create(int size);
	  #+end_src
	  该函数生成一个epoll专用的文件描述符，其中的参数是指定生成描述符的最大范围。
	* epoll_ctl
	  #+begin_src c
        int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
	  #+end_src
	  该函数用于控制某个文件描述符上的事件，可以注册事件，修改事件，删除事件。参数描述如下：
      |-------+-------------------------------------------------------------------------------------|
      | 参数  | 作用                                                                                |
      |-------+-------------------------------------------------------------------------------------|
      | epfd  | 由 epoll_create 生成的 epoll 专用的文件描述符                                       |
      | op    | 要进行的操作，可能的取值 EPOLL_CTL_ADD 注册、EPOLL_CTL_MOD 修改、EPOLL_CTL_DEL 删除 |
      | fd    | 关联的文件描述符                                                                    |
      | event | 指向 epoll_event 的指针                                                             |
      |-------+-------------------------------------------------------------------------------------|
	* epoll_wait
	  #+begin_src c
        int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
	  #+end_src
	  该函数用于轮询 I/O 事件的发生。参数描述如下：
      |-----------+--------------------------------------------|
      | 参数      | 作用                                       |
      |-----------+--------------------------------------------|
      | epfd      | 由epoll_create 生成的epoll专用的文件描述符 |
      | events    | 用于回传代处理事件的数组                   |
      | maxevents | 每次能处理的事件数                         |
      | timeout   | 等待 I/O 事件发生的超时值                  |
      |-----------+--------------------------------------------|
	  该函数返回发生事件数。

    首先通过 create_epoll(maxfds) 来创建一个 epoll 的句柄，其中 maxfds 为你的 epoll 所支持的最大句柄数。这个函数会返回一个新的 epoll 句柄，之后的所有操作都将通过这个句柄来进行操作。在用完之后，记得用 close() 来关闭这个创建出来的 epoll 句柄。

    之后在你的网络主循环里面，调用 epoll_wait(int epfd, epoll_event events, int max_events, int timeout) 来查询所有的网络接口，看哪一个可以读，哪一个可以写。基本的语法为：
	#+begin_src c
      int nfds = epoll_wait(kdpfd, events, maxevents, -1);
	#+end_src
	其中 kdpfd 为用 epoll_create 创建之后的句柄，events是一个 epoll_event 的指针，当 epoll_wait 函数操作成功之后，events 里面将储存所有的读写事件。max_events 是当前需要监听的所有 socket 句柄数。最后一个 timeout 参数指示 epoll_wait  的超时条件，为 0 时表示马上返回；为 -1 时表示函数会一直等下去直到有事件返回；为任意正整数时表示等这么长的时间，如果一直没有事件，则会返回。一般情况下如果网络主循环是单线程的话，可以用 -1 来等待，这样可以保证一些效率，如果是和主循环在同一个线程的话，则可以用 0 来保证主循环的效率。epoll_wait 返回之后，应该进入一个循环，以便遍历所有的事件。

    对epoll 的操作就这么简单，总共不过 4 个API：epoll_create, epoll_ctl, epoll_wait 和 close。以下是 man 中的一个例子。
	#+begin_src c++
      struct epoll_event ev, *events;
      for(;;)
      {
          nfds = epoll_wait(kdpfd, events, maxevents, -1); //等待 I/O 事件
          for(n = 0; n < nfds; ++n)
          {
              if(events[n].data.fd == listener) // 如果是主 socket 的事件，则表示有新连接进入，需要进行新连接的处理。
              {
                  client = accept(listener, (struct sockaddr *)&local,  &addrlen);
                  if(client < 0)
                  {
                      perror("accept error");
                      continue;
                  }
                  setnonblocking(client); // 将新连接置于非阻塞模式
                  ev.events = EPOLLIN | EPOLLET;
                  // 注意这里的参数 EPOLLIN | EPOLLET 并没有设置对写 socket 的监听，
                  // 如果有写操作的话，这个时候 epoll 是不会返回事件的，
                  // 如果要对写操作也监听的话，应该是 EPOLLIN | EPOLLOUT | EPOLLET。
                  ev.data.fd = client; // 并且将新连接也加入 EPOLL 的监听队列
                  if (epoll_ctl(kdpfd, EPOLL_CTL_ADD, client, &ev) < 0) // 设置好event之后，将这个新的event通过epoll_ctl
                  {
                      // 加入到 epoll 的监听队列里，
                      // 这里用 EPOLL_CTL_ADD 来加一个新的 epoll 事件。
                      // 可以通过 EPOLL_CTL_DEL 来减少一个 epoll 事件，
                      // 通过 EPOLL_CTL_MOD 来改变一个事件的监听方式。
                      fprintf(stderr, "epoll set insertion error: fd=%d", client);
                      return -1;
                  }
              }
              else // 如果不是主socket的事件的话，则代表这是一个用户的socket的事件，
              {
                  do_use_fd(events[n].data.fd);
              }
          }
      }
	#+end_src
*** Linux 下 epoll 编程实例
	epoll 模型主要负责对大量并发用户的请求进行及时处理，完成服务器与客户端的数据交互。其具体的实现步骤如下：
	1. 使用 epoll_create() 函数创建文件描述，设定可管理的最大 socket 描述符数目。
	2. 创建与 epoll 关联的接收线程，应用程序可以创建多个接收线程来处理 epoll 上的读通知事件，线程的数量依赖于程序的具体需要。
	3. 创建一个侦听 socket 的描述符 ListenSock，并将该描述符设定为非阻塞模式，调用 listen() 函数在该套接字上侦听有无新的连接请求，在 epoll_event 结构中设置要处理的事件类型 EPOLLIN，工作方式为 EPOLLET，以提高工作效率，同时使用 epoll_ctl() 来注册事件，最后启动网络监视线程。
	4. 网络监视线程启动循环，epoll_wait() 等待 epoll 事件发生。
	5. 如果epoll事件表明有新的连接请求，则调用 accept() 函数，将用户 socket 描述符添加到 epoll_data 联合体，同时设定该描述符为非阻塞，并在 epoll_event 结构中设置要处理的事件类型为读和写，工作方式为 EPOLL_ET。
	6. 如果 epoll 事件表明 socket 描述符上有数据可读，则将该 socket 描述符加入可读队列，通知接收线程读入数据，并将接收到的数据放入到接收数据的链表中，经逻辑处理后，将反馈的数据包放入到发送数据链表中，等待由发送线程发送。
	例子代码：
	#+begin_src c++
      #include <iostream>
      #include <sys/socket.h>
      #include <sys/epoll.h>
      #include <netinet/in.h>
      #include <arpa/inet.h>
      #include <fcntl.h>
      #include <unistd.h>
      #include <stdio.h>

      #define MAXLINE 10
      #define OPEN_MAX 100
      #define LISTENQ 20
      #define SERV_PORT 5555
      #define INFTIM 1000

      void setnonblocking(int sock)
      {
          int opts;
          opts = fcntl(sock, F_GETFL);
          if(opts < 0)
          {
              perror("fcntl(sock,GETFL)");
              exit(1);
          }
          opts = opts | O_NONBLOCK;
          if(fcntl(sock, F_SETFL, opts) < 0)
          {
              perror("fcntl(sock,SETFL,opts)");
              exit(1);
          }
      }

      int main()
      {
          int i, maxi, listenfd, connfd, sockfd, epfd, nfds;
          ssize_t n;
          char line[MAXLINE];
          socklen_t clilen;
          struct epoll_event ev,events[20]; //声明epoll_event结构体的变量, ev用于注册事件, events数组用于回传要处理的事件
          epfd=epoll_create(256); //生成用于处理accept的epoll专用的文件描述符, 指定生成描述符的最大范围为256
          struct sockaddr_in clientaddr;
          struct sockaddr_in serveraddr;
          listenfd = socket(AF_INET, SOCK_STREAM, 0);
          setnonblocking(listenfd); //把用于监听的socket设置为非阻塞方式
          ev.data.fd=listenfd; //设置与要处理的事件相关的文件描述符
          ev.events=EPOLLIN | EPOLLET; //设置要处理的事件类型
          epoll_ctl(epfd,EPOLL_CTL_ADD,listenfd,&ev); //注册epoll事件
          bzero(&serveraddr, sizeof(serveraddr));
          serveraddr.sin_family = AF_INET;
          char *local_addr="200.200.200.204";
          inet_aton(local_addr,&(serveraddr.sin_addr));
          serveraddr.sin_port=htons(SERV_PORT);  //或者htons(SERV_PORT);
          bind(listenfd,(sockaddr *)&serveraddr, sizeof(serveraddr));
          listen(listenfd, LISTENQ);
          maxi = 0;

          for( ; ; )
          {
              nfds=epoll_wait(epfd,events,20,500); //等待epoll事件的发生
              for(i=0;i<nfds;++i) //处理所发生的所有事件
              {
                  if(events[i].data.fd==listenfd)    /**监听事件**/
                  {
                      connfd = accept(listenfd,(sockaddr *)&clientaddr, &clilen);
                      if(connfd<0)
                      {
                          perror("connfd<0");
                          exit(1);
                      }
                      setnonblocking(connfd); //把客户端的socket设置为非阻塞方式
                      char *str = inet_ntoa(clientaddr.sin_addr);
                      std::cout<<"connect from "<_u115 ? tr<<std::endl;
                      ev.data.fd=connfd; //设置用于读操作的文件描述符
                      ev.events=EPOLLIN | EPOLLET; //设置用于注测的读操作事件
                      epoll_ctl(epfd,EPOLL_CTL_ADD,connfd,&ev); //注册ev事件
                  }
                  else if(events[i].events&EPOLLIN)     /**读事件**/
                  {
                      if ( (sockfd = events[i].data.fd) < 0) continue;
                      if ( (n = read(sockfd, line, MAXLINE)) < 0)
                      {
                          if (errno == ECONNRESET)
                          {
                              close(sockfd);
                              events[i].data.fd = -1;
                          }
                          else
                          {
                              std::cout<<"readline error"<<std::endl;
                          }
                      }
                      else if (n == 0)
                      {
                          close(sockfd);
                          events[i].data.fd = -1;
                      }
                      ev.data.fd=sockfd; //设置用于写操作的文件描述符
                      ev.events=EPOLLOUT | EPOLLET; //设置用于注测的写操作事件
                      epoll_ctl(epfd,EPOLL_CTL_MOD,sockfd,&ev); //修改sockfd上要处理的事件为EPOLLOUT
                  }
                  else if(events[i].events&EPOLLOUT)    /**写事件**/
                  {
                      sockfd = events[i].data.fd;
                      write(sockfd, line, n);
                      ev.data.fd=sockfd; //设置用于读操作的文件描述符
                      ev.events=EPOLLIN | EPOLLET; //设置用于注册的读操作事件
                      epoll_ctl(epfd,EPOLL_CTL_MOD,sockfd,&ev); //修改sockfd上要处理的事件为EPOLIN
                  }
              }
          }
      }
	#+end_src
* 并发编程
** 并发编程基础
*** 多线程及并发
   	线程是操作系统进行作业调度的最小单位，也是进程内部的一条执行路径。与进程不同，线程并没有对操作系统的资源所有权，也就是说同一个进程内的多个线程对资源的访问权是共享的。一个进程中的所有线程共享一个地址空间或者诸如打开的文件之类的其他资源，一个进程对这些资源的任何修改，都会影响到本进程中其他线程的运行。因此，需要对多个线程的执行进行细致地设计，使它们能够互不干涉，并且不破坏共享的数据及资源。

   	在单处理器中的并发系统里，不同进程和线程之间的指令流是交替执行的，但由于调度系统及CPU时钟的配合，使得程序对外表现出一种同时执行的外部特征；而在并行多处理器系统中，指令流之间的执行则是重叠的。无论是交替执行还是重叠执行，实际上并发程序都面临这同样的问题，即指令流的执行速度不可预测，这取决于其他指令流的活动状态、操作系统处理中断的方式及操作系统的调度策略。这给并发程序设计带来了如下的一些问题：
   	1. 多个进程（或线程）对同一全局资源的访问可能造成未定义的后果。例如，如果两个并发线程都使用同一个全局变量，并且都对该变量执行读写操作，那么程序执行的结果就取决于不同的读写执行顺序――而这些读写执行顺序是不可预知的。
   	2. 操作系统难以对资源进行最优化分配。这涉及到死锁及饥饿的问题。
   	3. 很难定位程序的错误。在多数情况下，并发程序设计的失误都是很难复现的，在一次执行中出现一种结果，而在下一次执行中，往往会出现迥然不同的其他结果。
   	因此，在进行多线程程序或者并发程序的设计时，尤其需要小心。可以看到的是，绝大多数并发程序的错误都出现在对共享资源的访问上，因此，如何保证对共享资源的访问以一种确定的、我们可以预知的方式运行，成为并发程序设计的首要问题。在操作系统领域，对共享资源的访问有个专用的数据，称为临界区。

   	临界区是一段代码，在这段代码中，进程将访问共享资源。当另外一个进程已经在这段代码中执行时，这个进程就不能在这段代码中执行。

   	也就是说，临界区是一个代码段，这个代码段不允许两条并行的指令流同时进入。提供这种保证的机制称为互斥：当一个进程在临界区访问共享资源时，其他进程不能进入该临界区。

*** 锁及互斥
   	实现互斥的机制，最重要的是互斥锁（mutex）。互斥锁实际上是一种二元信号量（只有0和1），专用于多任务之间临界区的互斥操作。（关于信号量及互斥锁的区别，可以参看操作系统相关知识）

   	mutex本质上是一个信号量对象，只有0和1两个值。同时，mutex还对信号量加1和减1的操作进行了限制，即某个线程对其进行了+1操作，则-1操作也必须由这个线程来完成。mutex的两个值也分别代表了mutex的两种状态。值为0, 表示锁定状态，当前对象被锁定，用户进程/线程如果试图Lock临界资源，则进入排队等待；值为1，表示空闲状态，当前对象为空闲，用户进程/线程可以Lock临界资源，之后mutex值减1变为0。

   	mutex可以抽象为创建（Create），加锁（Lock），解锁（Unlock），及销毁（Destroy）等四个操作。在创建mutex时，可以指定锁的状态是空闲或者是锁定，在linux中，这个属性的设置主要通过pthread\_mutex\_init来实现：

   	在使用mutex的时候，务必需要了解其本质：mutex实际上是一个在多个线程之间共享的信号量，当其进入锁定状态时，再试图对其加锁，则会阻塞线程。例如，对于两个线程A和B，其指令序列如下：
   	线程A
   	1. lock(&mutex)
   	2. do something
   	3. unlock(&mutex)
   	线程B
   	1. lock(&mutex)
   	2. do something
   	3. unlock(&mutex)
   	在线程A的语句1处，线程A对mutex进行了加锁操作，mutex变为锁定状态。在线程A的语句2及线程B的语句1处，A尚未对mutex进行解锁，而B则试图对mutex进行加锁操作，因此线程B被阻塞，直到A的语句3处，线程A对mutex进行了解锁，B的语句1才得以继续执行，将mutex进行加锁并继续执行语句2和语句3。因此，如果在do something中有对共享资源的访问操作，那么do something就是一个临界区，每次都只有一个线程能够进入这段代码。

*** 原子操作
   	无论是信号量，还是互斥锁，其中最重要的一个概念就是原子操作。所谓原子操作，就是不会被线程调度机制所打断的操作――从该操作的第一条指令开始到最后一条指令结束，中间不会有任何的上下文切换（context switch）。

   	在单处理器系统上，原子操作的实现较为简单：第一种方式是一些单指令即可完成的操作，如compare and swap、test and set等，由于上下文切换只可能出现在指令之间，因此单处理器系统上的单指令操作都是原子操作；另一种方式则是禁用中断，通过汇编语言支持，在指令执行期间，禁用处理器的所有中断操作，由于上下文切换都是通过中断来触发的，因此禁用中断后，可以保证指令流的执行不会被外部指令所打断。

   	而在多处理器系统上，情况要复杂一些。由于系统中有多个处理器在独立地运行，即使能在单条指令中完成的操作也有可能受到干扰。如，在不同的CPU运行的两个线程都在执行一条递减指令，即对内存中某个内存单元的值-1，则指令流水线可能是这样：（省略了取指）
   	* A处理器： |--读内存--|--计数减1--|--写内存 --|
   	* B处理器：            |--读内存 --|--计数减1--|--写内存--|
   	假设原来内存单元中存储的值为5，那么：A、B处理器所读到的内存值都为5，其往内存单元中写入的值都为4。因此，虽然进行了两次-1操作，但实际上运行的结果和执行了1次是一样的。

   	注：这是一个数据相关问题（关于数据相关问题，可以参考计算机体系结构中指令流水线的设计及数据相关的避免等资料），在单处理机中，这个问题可以通过检查处理机中的指令寄存器，来检查在流水线中的指令之间的相关性，如果出现数据相关的情况，可以通过延迟相关指令执行的方法来规避；而在对称多处理机中，由于CPU之间相互不知道对方的指令寄存器状态，那么这种流水线作业引起的数据竞跑就无法避免。

   	为了对原子操作提供支持，在 x86 平台上，CPU提供了在指令执行期间对总线加锁的手段。CPU芯片上有一条引线#HLOCK pin，如果汇编语言的程序中在一条指令前面加上前缀"LOCK"，经过汇编以后的机器代码就使CPU在执行这条指令的时候把#HLOCK pin的电位拉低，持续到这条指令结束时放开，从而把总线锁住，这样同一总线上别的CPU就暂时不能通过总线访问内存了，保证了这条指令在多处理器环境中的原子性。

   	可以看出，其实pthread\_mutex\_lock及pthread\_mutex\_unlock就是一个原子操作。它保证了两个线程不会同时对某个mutex变量加锁或者解锁，否则的话，互斥也就无从实现了。

   	i++和++i是原子操作吗？

   	有一个很多人也许都不是很清楚的问题：i++或++i是一个原子操作吗？在上一节，其实已经提到了，在SMP（对称多处理器）上，即使是单条递减汇编指令，其原子性也是不能保证的。那么在单处理机系统中呢？

   	在编译器对C/C++源代码进行编译时，往往会进行一些代码优化。例如，对i++这条指令，实际上编译器编译出的汇编代码是类似下面的汇编语句：
   	1. mov eax,[i]
   	2. add eax,1
   	3. mov [i],eax
   	语句1是将i所在的内存读取到寄存器中，而语句2是将寄存器的值加1，语句3是将寄存器值写回到内存中。之所以进行这样的操作，是为了CPU访问数据效率的高效。可以看出，i++是由一条语句被编译成了3条指令，因此，即使在单处理机系统上，i++这种操作也不是原子的。这是由于指令之间的乱序执行而造成的。

*** GCC的内建原子操作
	在GCC中，从版本4.1.2起，提供了\_\_sync\_*系列的built-in函数，用于提供加减和逻辑运算的原子操作。这些操作通过锁定总线，无论在单处理机和多处理机上都保证了其原子性。GCC提供的原子操作主要包括：
	#+begin_src c++
      type __sync_fetch_and_add (type *ptr, type value, ...);
      type __sync_fetch_and_sub (type *ptr, type value, ...);
      type __sync_fetch_and_or (type *ptr, type value, ...);
      type __sync_fetch_and_and (type *ptr, type value, ...);
      type __sync_fetch_and_xor (type *ptr, type value, ...);
      type __sync_fetch_and_nand (type *ptr, type value, ...);
	#+end_src
	这六个函数的作用是：取得ptr所指向的内存中的数据,同时对ptr中的数据进行修改操作(加,减,或,与,异或,与后取非)等。
	#+begin_src c++
      type __sync_add_and_fetch (type *ptr, type value, ...);
      type __sync_sub_and_fetch (type *ptr, type value, ...);
      type __sync_or_and_fetch (type *ptr, type value, ...);
      type __sync_and_and_fetch (type *ptr, type value, ...);
      type __sync_xor_and_fetch (type *ptr, type value, ...);
      type __sync_nand_and_fetch (type *ptr, type value, ...);
	#+end_src
	这六个函数与上六个函数基本相同，不同之处在于，上六个函数返回值为修改之前的数据，而这六个函数返回的值为修改之后的数据。
	#+begin_src c++
      bool __sync_bool_compare_and_swap (type *ptr, type oldval type newval, ...);
      type __sync_val_compare_and_swap (type *ptr, type oldval type newval, ...);
	#+end_src
	比较并交换指令。如果ptr所指向的内存中的数据等于oldval，则设置其为newval，同时返回true；否则返回false。
	#+begin_src c++
      type __sync_lock_test_and_set (type *ptr, type value, ...);
	#+end_src
	测试并置位指令。
	#+begin_src c++
      void __sync_lock_release (type *ptr, ...);
	#+end_src
	将ptr设置为0。

	其中，这些操作的操作数（type）可以是1,2,4或8字节长度的int类型，即：
	1. int8\_t / uint8\_t
	2. int16\_t / uint16\_t
	3. int32\_t / uint32\_t
	4. int64\_t / uint64\_t

** 环形无锁队列
*** 环形无锁队列的实现
	数据结构定义：
	#+begin_src c++
      template class LockFreeQueue
      {
        private:
          ElementT *mArray;
          int mCapacity;
          int mFront;
          int mTail;
      }
	#+end_src
	由于出队操作是在队首进行，入队操作是在队尾进行，因此，我们可以尝试用mFront和mTail来实现多个线程之间的协调。这其中会用到CAS操作：

	入队操作伪码：
	#+begin_src c++
      ……

      do {
          获取当前的mTail的值：curTailIndex;
          计算新的mTail的值：newTailIndex = (newTailIndex + 1) % size;
      } while(!CAS(mTail, curTailIndex, newTailIndex));
      插入元素到curTailIndex;
	#+end_src
	其中的do-while循环实现的是一个忙式等待：线程试图获取当前的队列尾部空间的控制权；一旦获取成功，则向其中插入元素。

	但是这样出队的时候就出现了问题：如何判断队首的位置里是否有相应元素呢？仅使用mFront来判断是不行的,这只能保证出队进程不会对同一个索引位置进行出队操作，而不能保证mFront的位置中一定有有效的元素。因此，为了保证出队线程与入队线程之间的协调，需要在LockFreeQueue中添加一个标志数组：
	#+begin_src c++
      char *mFlagArray;
	#+end_src
	mFlagArray中的元素标记mArray中与之对应的元素位置是否有效。mFlagArray中的元素有4个取值：
	* 0表示对应的mArray中的槽位为空；
	* 1表示对应槽位已被申请，正在写入；
	* 2表示对应槽位中为有效的元素，可以对其进行出队操作；
	* 3则表示正在弹出操作。

	修改后的无锁队列的代码如下：
	#+begin_src c++
      template class LockFreeQueue
      {
        public:
          LockFreeQueue(int s = 0)
          {
              mCapacity = s;
              mFront = 0;
              mTail = 0;
              mSize = 0;
          }

          ~LockFreeQueue() {}

          /**
           ,* 初始化queue。分配内存，设定size
           ,* 非线程安全，需在单线程环境下使用
           ,*/
          bool initialize()
          {
              mFlagArray = new char[mCapacity];
              if (NULL == mFlagArray)
                  return false;
              memset(mFlagArray, 0, mCapacity);

              mArray = reinterpret_cast(new char[mCapacity * sizeof(ElementT)]);
              if (mArray == NULL)
                  return false;
              memset(mArray, 0, mCapacity * sizeof(ElementT));

              return true;
          }

          const int capacity(void) const
          {
              return mCapacity;
          }

          const int size(void) const
          {
              return mSize;
          }

          /**
           ,* 入队函数，线程安全
           ,*/
          bool push(const ElementT & ele)
          {
              if (mSize >= mCapacity)
                  return false;

              int curTailIndex = mTail;

              char *cur_tail_flag_index = mFlagArray + curTailIndex;

              //// 忙式等待
              // while中的原子操作：如果当前tail的标记为已占用(1)，则更新cur_tail_flag_index，继续循环；否则，将tail标记设为已经占用
              while (!__sync_bool_compare_and_swap(cur_tail_flag_index, 0, 1))
              {
                  curTailIndex = mTail;
                  cur_tail_flag_index = mFlagArray +  curTailIndex;
              }

              //// 两个入队线程之间的同步
              int update_tail_index = (curTailIndex + 1) % mCapacity;

              // 如果已经被其他的线程更新过，则不需要更新；
              // 否则，更新为 (curTailIndex+1) % mCapacity;
              __sync_bool_compare_and_swap(&mTail, curTailIndex, update_tail_index);

              // 申请到可用的存储空间
              ,*(mArray + curTailIndex) = ele;

              // 写入完毕
              __sync_fetch_and_add(cur_tail_flag_index, 1);

              // 更新size;入队线程与出队线程之间的协作
              __sync_fetch_and_add(&mSize, 1);
              return true;
          }

          /**
           ,* 出队函数，线程安全
           ,*/
          bool pop(ElementT *ele)
          {
              if (mSize <= 0)
                  return false;

              int cur_head_index = mFront;
              char *cur_head_flag_index = mFlagArray + cur_head_index;
              while (!__sync_bool_compare_and_swap(cur_head_flag_index, 2, 3))
              {
                  cur_head_index = mFront;
                  cur_head_flag_index = mFlagArray + cur_head_index;
              }

              // 取模操作可以优化
              int update_head_index = (cur_head_index + 1) % mCapacity;
              __sync_bool_compare_and_swap(&mFront, cur_head_index, update_head_index);
              ,*ele = *(mArray + cur_head_index);

              // 弹出完毕
              __sync_fetch_and_sub(cur_head_flag_index, 3);

              // 更新size
              __sync_fetch_and_sub(&mSize, 1);

              return true;
          }
        private:
          ElementT *mArray;
          int mCapacity; // 环形数组的大小
          int mSize; //队列中元素的个数
          int mFront;
          int mTail;
          char *mFlagArray; // 标记位，标记某个位置的元素是否被占用
      };
	#+end_src

*** 死锁及饥饿
	LockFreeQueue实现了基本的多线程之间的协调，不会存在多个线程同时对同一个资源进行操作的情况，也就不会产生数据竞跑，这保证了对于这个队列而言，基本的访问操作（出队、入队）的执行都是安全的，其结果是可预期的。

	在多线程环境下，LockFreeQueue会不会出现死锁的情况呢？死锁有四个必要条件：
	1. 对资源的访问是互斥的；
	2. 请求和保持请求；
	3. 资源不可剥夺；
	4. 循环等待。

	在LockFreeQueue中，所有的线程都是对资源进行申请后再使用，一个线程若申请到了资源（这里的资源主要指环形队列中的内存槽位），就会立即使用，并且在使用完后释放掉该资源。不存在一个线程使用A资源的同时去申请B资源的情况，因此并不会出现死锁。

	但LockFreeQueue可能出现饥饿状态。例如，对两个出队线程A、B，两者都循环进行出队操作。当队列中有元素时，A总能申请到这个元素并且执行到弹出操作，而B则只能在DeQueue函数的while循环中一直循环下去。

*** 一些优化
	对LockFreeQueue可以进行一些优化。比如：
	1. 对于环形数组大小，可以设定为2的整数倍，如1024。这样取模的操作即可以简化为与mCapacity-1的按位与操作。
	2. 忙式等待的时候可能会出现某个线程一直占用cpu的情况。此时可以使用sleep(0)，让该线程让出CPU时间片，从就绪态转为挂起态。
** TODO 并发式服务器中的网络I/O设计
*** 非阻塞+I/O复用模式实现单线程并发服务器
* Unix权限管理
** 概要
   权限管理实际就是身份认证和访问权限校验的过程，被授权的用户可获得相应的访问权限，反之，则否。

   在 Unix 中用到权限管理的场合包括：
   1. 登录
   2. 进程对文件的存取访问

   第1点暂不予讨论。此处针对第2点：
   1. 身份认证的过程实际就是：“系统”校验“主体”是否符合“客体”所期望的身份的过程。我们以进程读写文件为例，在“进程对文件的存取访问”的身份认证过程中，内核担当“系统”角色，进程为“主体”，文件则为“客体”。那么，认证的依据是什么？主客双方都需要提供某种类型的身份凭证！在“进程对文件的存取访问”的认证过程中，进程提供的身份凭证是有效用户ID和有效组ID；文件提供的凭证则是属主用户ID和属主组ID。
   2. 在提供差异化权限管理的系统中，“客体”对不同“主体”的开放程度可能是不同的。这就需要访问权限校验。访问权限校验是指：在指定身份下，“客体”是否可满足特定类型的访问申请。

** 进程权限
   任意Unix进程可有如下“身份”：
   1. 实际用户ID、有效用户ID、保存的设置用户ID
   2. 实际组ID、有效组ID、保存的设置组ID

   这些“身份“的访问控制函数总结如下：
   #+BEGIN_SRC c
	 #include <unistd.h>

	 uid_t getuid( ); // 获取实际用户ID
	 uid_t geteuid( ); // 获取有效用户ID
	 int setuid( uid_t uid ); // 设置实际用户ID和有效用户ID
	 int seteuid( uid_t uid ); // 设置有效用户ID

	 gid_t getgid( ); // 获取实际组ID
	 gid_t getegid( ); // 获取有效组ID
	 int setgid( gid_t gid ); // 设置实际组ID和有效组ID
	 int setegid( gid_t gid ); // 设置有效组ID
   #+END_SRC

   setuid 和 setgid 可按规则设置调用进程的实际和有效“身份”，这些规则如下：
   1. 以 setuid 为例：
	  1. 若调用进程具有超级用户特权[fn:1]，则将调用进程的实际用户ID、有效用户ID、保存的设置用户ID都设为uid
	  2. 若调用进程的实际用户ID或保存的设置用户ID等于参数uid，则将调用进程的有效用户ID设置为uid
   2. setgid 的规则与 setuid 类似
** 文件权限
   文件跟权限管理相关的两条属性分别是：
   1. 属主，它指明了文件的归属，包括：
	  1. 用户ID
	  2. 组ID
   2. 访问控制权限，它指明了文件对不同身份的进程所允许的访问操作，共包括三组：
	  1. 对所属用户所允许的访问操作： -rwx
	  2. 对所属组所允许的访问操作：   ----rwx
	  3. 对其他用户所允许的访问操作： -------rwx
** 进程对文件的存取访问的权限校验过程
   1. 若进程有效用户ID为0，则允许访问
   2. 若进程有效用户ID等于文件所属者用户ID，则按用户访问权限位进行访问
   3. 若进程有效组ID等于文件所属者组ID，则按组访问权限位进行访问
   4. 若以上都不匹配，则按其他用户访问权限位进行访问

* Footnotes

[fn:1] 即，调用进程的有效用户ID等于0
